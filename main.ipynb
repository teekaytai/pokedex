{"cells":[{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[],"source":["import os\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from tensorflow.keras import layers, models\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.applications import VGG16, DenseNet201\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["DATA_DIR_NAME = 'data'"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"data":{"text/plain":["['Abra',\n"," 'Aerodactyl',\n"," 'Alakazam',\n"," 'Arbok',\n"," 'Arcanine',\n"," 'Articuno',\n"," 'Beedrill',\n"," 'Bellsprout',\n"," 'Blastoise',\n"," 'Bulbasaur']"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["class_names = os.listdir(DATA_DIR_NAME)\n","class_names.sort()\n","num_classes = len(class_names)\n","class_names[:10]"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["image_paths = []\n","labels = []\n","for class_name in class_names:\n","    pokemon_dir = os.path.join(DATA_DIR_NAME, class_name)\n","    image_file_names = os.listdir(pokemon_dir)\n","    image_paths.extend(os.path.join(pokemon_dir, name) for name in image_file_names)\n","    labels.extend([class_name] * len(image_file_names))\n","\n","df = pd.DataFrame({'filename': image_paths, 'class': labels})"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["train_df, test_df = train_test_split(\n","    df,\n","    test_size=0.2,\n","    random_state=42,\n","    shuffle=True,\n","    stratify=labels\n",")"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 11173 non-validated image filenames belonging to 151 classes.\n","Found 2794 non-validated image filenames belonging to 151 classes.\n"]}],"source":["BATCH_SIZE = 64\n","\n","train_datagen = ImageDataGenerator(\n","    rescale=1.0/255,\n","    rotation_range=20,\n","    zoom_range=0.2,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest'\n",")\n","\n","train_generator = train_datagen.flow_from_dataframe(\n","    train_df,\n","    directory=DATA_DIR_NAME,\n","    target_size=(224, 224),\n","    batch_size=BATCH_SIZE,\n","    class_mode='categorical',\n","    validate_filenames=False\n",")\n","\n","test_datagen = ImageDataGenerator(rescale=1.0/255)\n","\n","test_generator = test_datagen.flow_from_dataframe(\n","    test_df,\n","    directory=DATA_DIR_NAME,\n","    target_size=(224, 224),\n","    class_mode='categorical',\n","    batch_size=BATCH_SIZE,\n","    validate_filenames=False\n",")"]},{"cell_type":"code","execution_count":28,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58889256/58889256 [==============================] - 4s 0us/step\n"]}],"source":["vgg16_base_model = VGG16(weights='imagenet', include_top=False)\n","vgg16_base_model.trainable = False\n","\n","vgg16_model = models.Sequential([\n","  vgg16_base_model,\n","  layers.Flatten(input_shape=vgg16_base_model.output_shape[1:]),\n","  layers.Dense(4096, activation='relu', kernel_initializer='he_normal'),\n","  layers.Dense(256, activation='relu', kernel_initializer='he_normal'),\n","  layers.Dense(num_classes, activation='softmax')\n","])"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n","                                                                 \n"," flatten (Flatten)           (None, 25088)             0         \n","                                                                 \n"," dense (Dense)               (None, 4096)              102764544 \n","                                                                 \n"," dense_1 (Dense)             (None, 256)               1048832   \n","                                                                 \n"," dense_2 (Dense)             (None, 151)               38807     \n","                                                                 \n","=================================================================\n","Total params: 118566871 (452.30 MB)\n","Trainable params: 118566871 (452.30 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}],"source":["vgg16_model.summary(expand_nested=True, show_trainable=True)"]},{"cell_type":"code","execution_count":27,"metadata":{},"outputs":[],"source":["vgg16_model.compile(\n","    loss='categorical_crossentropy',\n","    optimizer='adam',\n","    metrics=['accuracy']\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vgg16_model.fit(\n","    train_generator,\n","    validation_data=test_generator,\n","    epochs=30\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["VGG16_MODEL_PATH = 'models/vgg16_model.keras'\n","vgg16_model.save(VGG16_MODEL_PATH)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["densenet201_base_model = DenseNet201(weights='imagenet', include_top=False)\n","densenet201_base_model.trainable = False\n","\n","densenet201_model = models.Sequential([\n","  densenet201_base_model,\n","  layers.GlobalAveragePooling2D(),\n","  layers.Dense(num_classes, activation='softmax')\n","])"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["densenet201_model.compile(\n","    loss='categorical_crossentropy',\n","    optimizer='adam',\n","    metrics=['accuracy']\n",")"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["175/175 [==============================] - 262s 1s/step - loss: 3.6541 - accuracy: 0.2922 - val_loss: 2.2005 - val_accuracy: 0.5845\n","Epoch 2/20\n","175/175 [==============================] - 237s 1s/step - loss: 1.8536 - accuracy: 0.6612 - val_loss: 1.4098 - val_accuracy: 0.7212\n","Epoch 3/20\n","175/175 [==============================] - 230s 1s/step - loss: 1.2797 - accuracy: 0.7665 - val_loss: 1.1290 - val_accuracy: 0.7674\n","Epoch 4/20\n","175/175 [==============================] - 229s 1s/step - loss: 1.0122 - accuracy: 0.8104 - val_loss: 0.9973 - val_accuracy: 0.7709\n","Epoch 5/20\n","175/175 [==============================] - 235s 1s/step - loss: 0.8438 - accuracy: 0.8400 - val_loss: 0.8842 - val_accuracy: 0.8021\n","Epoch 6/20\n","175/175 [==============================] - 235s 1s/step - loss: 0.7351 - accuracy: 0.8586 - val_loss: 0.8176 - val_accuracy: 0.8135\n","Epoch 7/20\n","175/175 [==============================] - 233s 1s/step - loss: 0.6437 - accuracy: 0.8768 - val_loss: 0.7828 - val_accuracy: 0.8157\n","Epoch 8/20\n","175/175 [==============================] - 234s 1s/step - loss: 0.5754 - accuracy: 0.8866 - val_loss: 0.7463 - val_accuracy: 0.8210\n","Epoch 9/20\n","175/175 [==============================] - 233s 1s/step - loss: 0.5308 - accuracy: 0.8939 - val_loss: 0.7162 - val_accuracy: 0.8278\n","Epoch 10/20\n","175/175 [==============================] - 232s 1s/step - loss: 0.4857 - accuracy: 0.9051 - val_loss: 0.7008 - val_accuracy: 0.8311\n","Epoch 11/20\n","175/175 [==============================] - 236s 1s/step - loss: 0.4366 - accuracy: 0.9128 - val_loss: 0.6783 - val_accuracy: 0.8304\n","Epoch 12/20\n","175/175 [==============================] - 234s 1s/step - loss: 0.4058 - accuracy: 0.9178 - val_loss: 0.6643 - val_accuracy: 0.8357\n","Epoch 13/20\n","175/175 [==============================] - 233s 1s/step - loss: 0.3728 - accuracy: 0.9256 - val_loss: 0.6413 - val_accuracy: 0.8372\n","Epoch 14/20\n","175/175 [==============================] - 234s 1s/step - loss: 0.3474 - accuracy: 0.9305 - val_loss: 0.6444 - val_accuracy: 0.8375\n","Epoch 15/20\n","175/175 [==============================] - 234s 1s/step - loss: 0.3313 - accuracy: 0.9324 - val_loss: 0.6260 - val_accuracy: 0.8432\n","Epoch 16/20\n","175/175 [==============================] - 230s 1s/step - loss: 0.3077 - accuracy: 0.9415 - val_loss: 0.6289 - val_accuracy: 0.8393\n","Epoch 17/20\n","175/175 [==============================] - 229s 1s/step - loss: 0.2950 - accuracy: 0.9384 - val_loss: 0.6197 - val_accuracy: 0.8490\n","Epoch 18/20\n","175/175 [==============================] - 233s 1s/step - loss: 0.2757 - accuracy: 0.9447 - val_loss: 0.6290 - val_accuracy: 0.8429\n","Epoch 19/20\n","175/175 [==============================] - 237s 1s/step - loss: 0.2597 - accuracy: 0.9491 - val_loss: 0.6114 - val_accuracy: 0.8461\n","Epoch 20/20\n","175/175 [==============================] - 233s 1s/step - loss: 0.2488 - accuracy: 0.9493 - val_loss: 0.6327 - val_accuracy: 0.8379\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x7816defaa7a0>"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["densenet201_model.fit(\n","    train_generator,\n","    validation_data=test_generator,\n","    epochs=20\n",")"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["DENSENET201_MODEL_PATH = 'models/densenet201_model.keras'\n","densenet201_model.save(DENSENET201_MODEL_PATH)"]}],"metadata":{},"nbformat":4,"nbformat_minor":0}
