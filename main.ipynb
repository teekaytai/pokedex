{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import os\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from keras import layers, models\n","from keras.applications import *\n","from keras.callbacks import EarlyStopping\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","%matplotlib inline"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["DATA_DIR_NAME = 'data'"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/plain":["['Abra',\n"," 'Aerodactyl',\n"," 'Alakazam',\n"," 'Arbok',\n"," 'Arcanine',\n"," 'Articuno',\n"," 'Beedrill',\n"," 'Bellsprout',\n"," 'Blastoise',\n"," 'Bulbasaur']"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["class_names = os.listdir(DATA_DIR_NAME)\n","class_names.sort()\n","num_classes = len(class_names)\n","class_names[:10]"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["image_paths = []\n","labels = []\n","for class_name in class_names:\n","    pokemon_dir = os.path.join(DATA_DIR_NAME, class_name)\n","    image_file_names = os.listdir(pokemon_dir)\n","    image_paths.extend(os.path.join(pokemon_dir, name) for name in image_file_names)\n","    labels.extend([class_name] * len(image_file_names))\n","\n","df = pd.DataFrame({'filename': image_paths, 'class': labels})"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"data":{"text/plain":["class\n","Pikachu      286\n","Charizard    167\n","Venusaur     162\n","Sandslash    142\n","Gengar       140\n","            ... \n","Poliwrath     61\n","Nidoking      60\n","Dratini       57\n","Nidoran♂      50\n","Nidoran♀      44\n","Name: count, Length: 151, dtype: int64"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["df.value_counts('class')"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["train_df, test_df = train_test_split(\n","    df,\n","    test_size=0.2,\n","    random_state=42,\n","    shuffle=True,\n","    stratify=labels\n",")\n","train_df, validation_df = train_test_split(\n","    train_df,\n","    test_size=0.25,\n","    random_state=42,\n","    shuffle=True,\n","    stratify=labels\n",")"]},{"cell_type":"markdown","metadata":{},"source":["### Baseline Model"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Found 11172 validated image filenames belonging to 151 classes.\n","Found 2793 validated image filenames belonging to 151 classes.\n"]}],"source":["BATCH_SIZE = 64\n","vgg16_input_size = (224, 224)\n","\n","datagen = ImageDataGenerator(preprocessing_function=vgg16.preprocess_input)\n","\n","train_generator = datagen.flow_from_dataframe(\n","    train_df,\n","    target_size=vgg16_input_size,\n","    class_mode='categorical',\n","    batch_size=BATCH_SIZE\n",")\n","\n","validation_generator = datagen.flow_from_dataframe(\n","    validation_df,\n","    target_size=vgg16_input_size,\n","    class_mode='categorical',\n","    batch_size=BATCH_SIZE\n",")"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["vgg16_base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n","vgg16_base_model.trainable = False\n","\n","vgg16_model = models.Sequential([\n","  vgg16_base_model,\n","  layers.Flatten(input_shape=vgg16_base_model.output_shape[1:]),\n","  layers.Dense(4096, activation='relu'),\n","  layers.Dense(num_classes, activation='softmax')\n","])\n","\n","vgg16_model.compile(\n","    loss='categorical_crossentropy',\n","    optimizer='adam',\n","    metrics=['accuracy']\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["early_stopping = EarlyStopping(\n","    monitor='val_loss',\n","    min_delta=0.001,\n","    patience=5,\n","    verbose=1,\n","    restore_best_weights=True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vgg16_model.fit(\n","    train_generator,\n","    validation_data=validation_generator,\n","    epochs=30\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["VGG16_MODEL_PATH = 'models/vgg16_model.keras'\n","vgg16_model.save(VGG16_MODEL_PATH)"]},{"cell_type":"code","execution_count":29,"metadata":{},"outputs":[],"source":["densenet201_base_model = DenseNet201(weights='imagenet', include_top=False)\n","densenet201_base_model.trainable = False\n","\n","densenet201_model = models.Sequential([\n","  densenet201_base_model,\n","  layers.GlobalAveragePooling2D(),\n","  layers.Dense(num_classes, activation='softmax')\n","])"]},{"cell_type":"code","execution_count":30,"metadata":{},"outputs":[],"source":["densenet201_model.compile(\n","    loss='categorical_crossentropy',\n","    optimizer='adam',\n","    metrics=['accuracy']\n",")"]},{"cell_type":"code","execution_count":31,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/20\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/PIL/Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["175/175 [==============================] - 262s 1s/step - loss: 3.6541 - accuracy: 0.2922 - val_loss: 2.2005 - val_accuracy: 0.5845\n","Epoch 2/20\n","175/175 [==============================] - 237s 1s/step - loss: 1.8536 - accuracy: 0.6612 - val_loss: 1.4098 - val_accuracy: 0.7212\n","Epoch 3/20\n","175/175 [==============================] - 230s 1s/step - loss: 1.2797 - accuracy: 0.7665 - val_loss: 1.1290 - val_accuracy: 0.7674\n","Epoch 4/20\n","175/175 [==============================] - 229s 1s/step - loss: 1.0122 - accuracy: 0.8104 - val_loss: 0.9973 - val_accuracy: 0.7709\n","Epoch 5/20\n","175/175 [==============================] - 235s 1s/step - loss: 0.8438 - accuracy: 0.8400 - val_loss: 0.8842 - val_accuracy: 0.8021\n","Epoch 6/20\n","175/175 [==============================] - 235s 1s/step - loss: 0.7351 - accuracy: 0.8586 - val_loss: 0.8176 - val_accuracy: 0.8135\n","Epoch 7/20\n","175/175 [==============================] - 233s 1s/step - loss: 0.6437 - accuracy: 0.8768 - val_loss: 0.7828 - val_accuracy: 0.8157\n","Epoch 8/20\n","175/175 [==============================] - 234s 1s/step - loss: 0.5754 - accuracy: 0.8866 - val_loss: 0.7463 - val_accuracy: 0.8210\n","Epoch 9/20\n","175/175 [==============================] - 233s 1s/step - loss: 0.5308 - accuracy: 0.8939 - val_loss: 0.7162 - val_accuracy: 0.8278\n","Epoch 10/20\n","175/175 [==============================] - 232s 1s/step - loss: 0.4857 - accuracy: 0.9051 - val_loss: 0.7008 - val_accuracy: 0.8311\n","Epoch 11/20\n","175/175 [==============================] - 236s 1s/step - loss: 0.4366 - accuracy: 0.9128 - val_loss: 0.6783 - val_accuracy: 0.8304\n","Epoch 12/20\n","175/175 [==============================] - 234s 1s/step - loss: 0.4058 - accuracy: 0.9178 - val_loss: 0.6643 - val_accuracy: 0.8357\n","Epoch 13/20\n","175/175 [==============================] - 233s 1s/step - loss: 0.3728 - accuracy: 0.9256 - val_loss: 0.6413 - val_accuracy: 0.8372\n","Epoch 14/20\n","175/175 [==============================] - 234s 1s/step - loss: 0.3474 - accuracy: 0.9305 - val_loss: 0.6444 - val_accuracy: 0.8375\n","Epoch 15/20\n","175/175 [==============================] - 234s 1s/step - loss: 0.3313 - accuracy: 0.9324 - val_loss: 0.6260 - val_accuracy: 0.8432\n","Epoch 16/20\n","175/175 [==============================] - 230s 1s/step - loss: 0.3077 - accuracy: 0.9415 - val_loss: 0.6289 - val_accuracy: 0.8393\n","Epoch 17/20\n","175/175 [==============================] - 229s 1s/step - loss: 0.2950 - accuracy: 0.9384 - val_loss: 0.6197 - val_accuracy: 0.8490\n","Epoch 18/20\n","175/175 [==============================] - 233s 1s/step - loss: 0.2757 - accuracy: 0.9447 - val_loss: 0.6290 - val_accuracy: 0.8429\n","Epoch 19/20\n","175/175 [==============================] - 237s 1s/step - loss: 0.2597 - accuracy: 0.9491 - val_loss: 0.6114 - val_accuracy: 0.8461\n","Epoch 20/20\n","175/175 [==============================] - 233s 1s/step - loss: 0.2488 - accuracy: 0.9493 - val_loss: 0.6327 - val_accuracy: 0.8379\n"]},{"data":{"text/plain":["<keras.src.callbacks.History at 0x7816defaa7a0>"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["densenet201_model.fit(\n","    train_generator,\n","    validation_data=validation_generator,\n","    epochs=20\n",")"]},{"cell_type":"code","execution_count":32,"metadata":{},"outputs":[],"source":["DENSENET201_MODEL_PATH = 'models/densenet201_model.keras'\n","densenet201_model.save(DENSENET201_MODEL_PATH)"]}],"metadata":{"kernelspec":{"display_name":"COMP9444","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.18"}},"nbformat":4,"nbformat_minor":0}
